<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Paper 01</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header class="main-header">Main Header</header>
    <div class="sub-header sticky">Download Source | Publisher: IEEE | Model Architecture: DenseNet | Dataset: CIFAR-10, CIFAR-100, SVHN, and ImageNet</div>
    <main class="content">
        <div class="left-section">
            <ul class="paper-options">
                <li><strong>Summary:</strong>
                    <p>Focused on Problems like vanishing gradient and overfitting in deeper CNN architecture</p>
                     <p>Proposed DenseNet architecture inspired from following concepts:</p>
                        <ul class="paper-options">
                            <li> <strong>Highway Networks:</strong> It made feasible to train neural networks with more than 100 layers end-to-end. They achieve this by incorporating bypassing paths and gating units.</li>
                            <li> <strong>Residual Mappings in ResNets:</strong> ResNets also have bypassing paths but simplify it further by introducing pure identity mappings as bypass connections. Unlike Highway Networks, ResNets don't require gating units. Instead, they directly add the output of a layer to its input, forming residual mappings.</li>
                            <li> <strong>Stochastic Depth in ResNets:</strong> It assumes that not all layers are essential during training. So, it randomly drops layers during training. This forces the network to learn alternative pathways and reduces reliance on any single layer.</li>
                            <li> <strong>ResNets with Pre-Activation:</strong> In residual block, activation function (e.g., ReLU) is applied before the convolutional layers.</li>
                        </ul>
                    <p>DenseNet used the idea of bypassing paths further by ensuring that information is passed not just to the next layer, but to all subsequent layers.</p>
                    <p>ResNets combine layer outputs via summation, DenseNet uses concatenation instead.</p>
                    <p>DenseNet addresses the issue of redundancy. Unlike typical CNN where layers learn redundant features repeatedly, DenseNet passes all previous layer outputs directly to the current layer which reduces the total number of parameters needed. For this reason DenseNet layers are very narrow (e.g., 12 filters per layer) compared to traditional architectures.</p>
                    <p>DenseNet focuses on depth and connectivity for efficient feature reuse as opposed to wide architectures like GoogLeNet, Wide ResNets, and FractalNets having more number of parameters expand the networkâ€™s capacity to process diverse features at each stage.</p>
                </li>
            </ul>
        </div>
    </main>
</body>
</html>
