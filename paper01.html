<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Paper Details: Densely Connected Convolutional Networks</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header onclick="location.href='index.html';">
        <h1>Paper Details</h1>
    </header>
    <main>
        <section>
            <ul class="paper-options">
                <li><strong>PDF:</strong> <a href="https://drive.google.com/file/d/1BRmX_KuEjDHt8j59i6fiYauwV5xTaYaJ/view?usp=drive_link" target="_blank">Download</a>
                                </strong> <a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html" target="_blank">View Source</a>
                </li>
                <li><strong>Publisher:</strong> IEEE</li>
                <li><strong>Summary:</strong>
                    <p>Focused on Problems like vanishing gradient and overfitting in deeper CNN architecture</p>
                     <p>Proposed DenseNet architecture inspired from following concepts:
                     <ul>Highway Networks: It made feasible to train neural networks with more than 100 layers end-to-end.
                                           They achieve this by incorporating bypassing paths and gating units.</ul>
                     <ul>Residual Mappings in ResNets: ResNets also hev bypassing paths but simplify it further by introducing pure 
                                  identity mappings as bypass connections. Unlike Highway Networks, ResNets don't require gating units. 
                                  Instead, they directly add the output of a layer to its input, forming residual mappings.</ul>
                     <ul>Stochastic Depth in ResNets: It assumes that not all layers are essential during training.
                                                      So, it randomly drops layers during training. 
                                                      This forces the network to learn alternative pathways and reduces reliance on any single layer</ul>
                    <ul>ResNets with Pre-Activation: In residual block, activation function (e.g., ReLU) is applied before the convolutional layers.</ul>
                    </p>
                    <p>DenseNet used the idea of bypassing paths further by ensuring that information is passed not just to the next layer, 
                        but to all subsequent layers.</p>
                    <p>ResNets combine layer outputs via summation, DenseNet uses concatenation instead.</p>
                <p>DenseNet addresses the issue of redundancy. Unlike typical CNN where layers learn redundant features repeatedly, 
                    DenseNet passes all previous layer outputs directly to the current layer which reduces the total number of parameters needed. For this reason 
                    DenseNet layers are very narrow (e.g., 12 filters per layer) compared to traditional architectures </p>
                    
                </li>
                <li><strong>Model Architecture:</strong>DenseNet</li>
                <li><strong>Dataset:</strong>CIFAR-10, CIFAR-100, SVHN, and ImageNet</li>
            </ul>
        </section>
    </main>
</body>
</html>
